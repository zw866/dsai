# üìå HOMEWORK

## Homework 3: AI Report Validation System

üïí *Estimated Time: 3-4 hours*

---

## üìã Homework Overview

Design and implement a **validation system** that performs qualitative content analysis on AI-generated reports. Unlike [`LAB_ai_quality_control.md`](../09_text_analysis/LAB_ai_quality_control.md), which uses predefined Likert scales, you will **customize your validator** to fit your specific use case and criteria. Then, run an **experiment** comparing different prompts (e.g., Prompt A, B, C) to show which performs significantly better using statistical analysis.

This homework primarily reflects on **Module 9: AI for Text Analysis**. You will design your own validation criteria, scales, and benchmarks, then use statistical methods (t-test, ANOVA, or regression) to compare prompt performance.

**Note**: This homework goes beyond the LAB by requiring you to customize validation criteria for your use case and conduct a statistical experiment comparing multiple prompts. You decide what scales to use, what benchmarks to compare against, and how to structure the evaluation.

---

## üìù Instructions

### Who?
Individual homework assignment - 1 per team member.

### What?
Design and implement a **validation system** that uses qualitative content analysis to evaluate AI-generated reports. **Customize your validator** to fit your use case (don't just use the Likert scales from the LAB). Then run an **experiment**: generate reports using multiple different prompts (e.g., Prompt A, B, C), collect validation scores many times for each prompt, and use statistical analysis (t-test, ANOVA, or regression) to show that one prompt performs significantly better than another. **Submit a single .docx file.**

### Why?
This homework demonstrates your ability to customize validation systems for specific use cases, design experiments comparing prompt performance, and use statistical methods to draw meaningful conclusions about AI-generated content quality.

---

## ‚úÖ Your Deliverable

### AI Report Validation System [100 pts]

Your deliverable should be a complete validation system that demonstrates:
- **Customized Validation Framework**: Clear criteria, scales, and benchmarks tailored to your use case (not just the LAB's Likert scales)
- **Qualitative Content Analysis**: Systematic evaluation using AI as the reviewer
- **Experimental Design**: Comparison of multiple prompts (e.g., Prompt A, B, C) with many validation scores collected for each
- **Statistical Analysis**: Use t-test, ANOVA, or regression to show that one prompt performs significantly better than another
- **Implementation**: Working system that validates reports and conducts the statistical experiment

**Requirements:**

- [ ] **üìù [30 pts] Writing Component**: Brief written explanation of your validation system (NOT AI-generated)
  - Explain the purpose and design of your validation system
  - Describe how you customized your validator (different from the LAB's Likert scales)
  - Describe your experimental design: What prompts did you compare? How many scores did you collect?
  - Present your statistical analysis results: Which prompt performed significantly better? Include your test statistic and p-value
  - Discuss design choices or challenges you encountered
  - Written in your own words (4+ paragraphs)

- [ ] **üîó [20 pts] Git Repository Links**: Working, valid links to relevant content in your git repository
  - Link to your validation system script/code
  - Link to the validation criteria/rubric definition (if separate file)
  - Link to example validation outputs or results
  - Link to the reports you validated (from Homework 1 or 2)
  - Links must be functional and point to the correct files

- [ ] **üì∏ [25 pts] Screenshots/Outputs**: Screenshots and/or samples of outputs
  - Screenshot showing your validation system in action
  - Screenshot or sample of validation results for one evaluated report
  - Screenshot showing your validation criteria/rubric (if visual)
  - Screenshot of your statistical analysis results (t-test, ANOVA, or regression output)
  - Screenshot showing comparison of scores across different prompts (e.g., boxplot, summary statistics)
  - At least 4-5 screenshots total

- [ ] **üìö [25 pts] Documentation**: Brief documentation for your validation system
  - **Validation Criteria Table**: Table summarizing each evaluation dimension: Dimension name, description, scale/measurement method, and benchmark (if applicable). Explain how your criteria differ from the LAB's Likert scales.
  - **Experimental Design**: Description of your experiment: What prompts did you compare? How many validation scores did you collect per prompt? What was your sample size?
  - **Statistical Analysis**: Description of your statistical test (t-test, ANOVA, or regression), including your hypothesis, test results, and interpretation
  - **System Design**: Description of how your validation system works, including the AI reviewer's role
  - **Technical Details**: Any information needed to understand your software (e.g., API keys, packages, file structure)
  - **Usage Instructions**: How to install dependencies, set up the system, run validations, and conduct the experiment. Make it easy for me!

**Total: 100 pts**

---

## üì§ To Submit

- For credit: Submit all four required components listed in the **Requirements** section above (100 pts total). **Submit a single .docx file.**

Submit via Canvas by the due date specified in the course schedule.

---

![](../../docs/images/homework.png)

---

‚Üê üè† [Back to Top](#HOMEWORK)
